{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Large Notebook 3: Reinfocement Learning\n",
    "## Ex. 3: Control problem MountainCar-v0\n",
    "\n",
    "In the exercise class we will cover the control problem of a car at the bottom of a valley which should pick-up enough momentum to get over the hill. We will use the environment from the OpenAI Gym, which allows you to play and visualize the 'game'. Use RL to train a policy that gets the car over the hill in the least amount of time. \n",
    "\n",
    "**Before you can start this exercise you have to install the package OpenAI Gym. Start your anaconda environment with python3 and install:**\n",
    "\n",
    "* pip install gymnasium[classic-control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[classic-control]\n",
      "  Obtaining dependency information for gymnasium[classic-control] from https://files.pythonhosted.org/packages/c1/2a/f931b9b7515c16e5285ecb0e6c2d773266a4f781332f24456e7c0d516c8d/gymnasium-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading gymnasium-1.1.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mardeen\\desktop\\ut\\machine-learning\\my_env\\lib\\site-packages (from gymnasium[classic-control]) (2.2.2)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium[classic-control])\n",
      "  Obtaining dependency information for cloudpickle>=1.2.0 from https://files.pythonhosted.org/packages/7e/e8/64c37fadfc2816a7701fa8a6ed8d87327c7d54eacfbfb6edab14a2f2be75/cloudpickle-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mardeen\\desktop\\ut\\machine-learning\\my_env\\lib\\site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
      "  Obtaining dependency information for farama-notifications>=0.0.1 from https://files.pythonhosted.org/packages/05/2c/ffc08c54c05cdce6fbed2aeebc46348dbe180c6d2c541c7af7ba0aa5f5f8/Farama_Notifications-0.0.4-py3-none-any.whl.metadata\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\n",
      "  Obtaining dependency information for pygame>=2.1.3 from https://files.pythonhosted.org/packages/d2/55/ca3eb851aeef4f6f2e98a360c201f0d00bd1ba2eb98e2c7850d80aabc526/pygame-2.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pygame-2.6.1-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading pygame-2.6.1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.6 MB 1.2 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.1/10.6 MB 1.1 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/10.6 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/10.6 MB 1.5 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/10.6 MB 1.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/10.6 MB 1.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.6/10.6 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/10.6 MB 1.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/10.6 MB 1.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.9/10.6 MB 1.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/10.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.2/10.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.3/10.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/10.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.6/10.6 MB 2.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/10.6 MB 2.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.9/10.6 MB 2.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.0/10.6 MB 2.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.2/10.6 MB 2.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.3/10.6 MB 2.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.5/10.6 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/10.6 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.7/10.6 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.0/10.6 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.2/10.6 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.4/10.6 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.6/10.6 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/10.6 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.9/10.6 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.1/10.6 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.3/10.6 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.5/10.6 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.6/10.6 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.8/10.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.1/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.3/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.4/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.6/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.7/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.8/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.9/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.2/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.4/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.5/10.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.6/10.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.7/10.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.6 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.9/10.6 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.0/10.6 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.1/10.6 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.2/10.6 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.2/10.6 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.3/10.6 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.4/10.6 MB 2.8 MB/s eta 0:00:02Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "   ---------------------------- ----------- 7.5/10.6 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.7/10.6 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.0/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.2/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.5/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.8/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.1/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.8/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.3/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading gymnasium-1.1.0-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 143.4/965.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 256.0/965.5 kB 3.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 430.1/965.5 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 542.7/965.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 686.1/965.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 870.4/965.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  962.6/965.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 965.5/965.5 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: farama-notifications, pygame, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.1.1 farama-notifications-0.0.4 gymnasium-1.1.0 pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT IF GYMNASIUM FAILS TO IMPORT\n",
    "#%pip install gymnasium[classic-control]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have properly installed the openAI gym you should be able to import it. We will now run a DEMO to see if everything is working. The code already is able to simulate the MountainCar problem for the case that it actions are **random**. To be able to view the rendered video of the poor and helpless car, desperately trying to drive up the hill, you should run the code on your own computer.\n",
    "For more info on this particular environment see e.g. the website: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    \"\"\"run the MountainCar environment with random actions\"\"\"\n",
    "    \n",
    "    env = gym.make('MountainCar-v0', render_mode='human')  #  create an instance of the environment\n",
    "\n",
    "    state = env.reset()  # reset the current game\n",
    "\n",
    "    for _ in range(200):  # play 200 random actions\n",
    "        env.render()  # render the current game state to screen\n",
    "        a = env.action_space.sample()  # get a random action\n",
    "        state, reward, terminated, truncated, info = env.step(a) # take the action and return the outcome\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "# run the demo\n",
    "demo() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your RL player, ie. training your policy.\n",
    "We have to start by creating the game environment and checking some properties of the state and action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state: (array([-0.5595893,  0.       ], dtype=float32), {})\n",
      "Number of sctions in the action space: 3\n",
      "Lowest state in the state space: [-1.2  -0.07]\n",
      "Hightest state in the state space: [0.6  0.07]\n",
      "After the step with (a=1): [-5.5931991e-01  2.6940505e-04] -1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')  # no rendering!\n",
    "\n",
    "# get usefull information about the environment:\n",
    "state = env.reset()\n",
    "print('start state:', state)\n",
    "print('Number of sctions in the action space:', env.action_space.n)\n",
    "print('Lowest state in the state space:', env.observation_space.low)\n",
    "print('Hightest state in the state space:', env.observation_space.high)\n",
    "\n",
    "#perform one step of the game for action a=1\n",
    "a=1\n",
    "state, reward, terminated, truncated, info = env.step(a)\n",
    "print('After the step with (a=1):',state, reward, terminated, truncated, info )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the car starts out in state with two floats [-0.525, 0] (as it initializes random these numbers will differ each time you reset). You can perform any of 3 actions (a = 0 or 1 or 2). We don't know what the numbers in the state mean, they could be the $x$, $y$ coordinates of the car or the velocity and height, but **we also don't have to know!** We will let the RL algortithm learn how to drive the car regardless of the exact meaning of the state.\n",
    "\n",
    "You should now code a function `s2q(s)` that links state `s` to a location in the Q-matrix. This can quickly be done by discretizing the state space into bins and determine the bin number corresponding to a certain value. The function should return a tuple (or list) `loc` that holds the two bin numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2q(s):\n",
    "    # convert continous state values to discrete location indices inside the Q matrix\n",
    "    #----------ADD CODE HERE---------#\n",
    "    print(\"state:\",state)\n",
    "    \n",
    "    n_bins_pos = 10\n",
    "    n_bins_vel = 10\n",
    "    \n",
    "    pos_min,pos_max = -1.2, 0.6\n",
    "    vel_min,vel_max = -0.07,0.07\n",
    "    \n",
    "    # bin width for each position\n",
    "    bin_width_pos = (pos_max-pos_min) /n_bins_pos\n",
    "    bin_width_vel = (vel_max-vel_min) /n_bins_vel\n",
    "    \n",
    "    #pos, vel = s\n",
    "    pos = s[0]\n",
    "    vel = s[1]\n",
    "    \n",
    "    \n",
    "    #bins indices\n",
    "    print(\"pos\",pos,\"posmin\",pos_min,\"bin\",bin_width_pos,\"\\n\")\n",
    "    pos_index = (pos-pos_min) / bin_width_pos\n",
    "    print(\"posindex,\",pos_index)\n",
    "    pos_index = int(pos_index)\n",
    "    vel_index = int((vel-vel_min) / bin_width_vel )\n",
    "    \n",
    "    #clipping\n",
    "    position_index = min(max(pos_index, 0), n_bins_pos - 1)\n",
    "    velocity_index = min(max(vel_index, 0), n_bins_vel - 1)\n",
    "\n",
    "    loc = (position_index,velocity_index)\n",
    "    \n",
    "    \n",
    "    return loc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function `qlearn()` should train your Q-matrix by playing `num_games` games according to an 'epsilon-greedy' strategy (Google it!) and update the Q-matrix accoding to the following Bellmann equation:\n",
    "\n",
    "$$ \\mathbf{Q}^{\\rm new}[s_t,a_t]=(1-\\alpha)\\mathbf{Q}[s_t,a_t]+\\alpha\\left(R_t+\\gamma\\, \\text{max}_a  \\mathbf{Q}[s_{t+1},a]\\right). $$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate and $\\gamma$ is the discount factor and are bounded by $\\alpha,\\gamma\\in[0,1]$. These parameters have to be set with care, as they influence the speed of convergence of the Q-matrix. The discount factor lets you weigh the importance of future over immediate rewards. This is done by mixing-in the term $\\text{max}_a  \\mathbf{Q}[s_{t+1},a]$, which gives the maximum Q value in the future state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearn(Q, α, γ, ϵ, ϵ_min, num_games):\n",
    "    \"\"\" \n",
    "    learns the Q table by interacting with the environment and applying the Bellman eqation \n",
    "    Q: q-table (n-dimensional ndarray)\n",
    "    α: learning rate\n",
    "    γ: discount factor\n",
    "    ϵ: probability of taking a random action in the epsilon-greedy policy\n",
    "    ϵ_min: minimum value ϵ can take when applying a reduction algortihm to ϵ\n",
    "    \"\"\"\n",
    "\n",
    "    state = env.reset()  # reset the current game\n",
    "    wins = 0\n",
    "    \n",
    "    #----------ADD CODE HERE---------#\n",
    "    for i in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.uniform(0,1) < ϵ: #epsilon greedy algorithm\n",
    "                action = env.action_space.sample() #take a random action\n",
    "            # else choose the best know from the Q table\n",
    "            else:\n",
    "                s_index = s2q(state)\n",
    "                action = np.argmax(Q[s_index])\n",
    "            \n",
    "            print(env.step(action))    \n",
    "            next_state, reward, terminated, truncated, info  = env.step(action)\n",
    "            \n",
    "            next_s_index = s2q(next_state)\n",
    "            s_index = s2q(state)\n",
    "            \n",
    "            #bellman equation\n",
    "            best_next_action = np.max(Q[next_s_index])\n",
    "            Q[s_index][action] = (1 - α) * Q[s_index][action] + α * (reward + γ * best_next_action)\n",
    "            \n",
    "            state=next_state\n",
    "            \n",
    "            if done and reward > 0:\n",
    "                wins+=1\n",
    "        \n",
    "        if ϵ > ϵ_min:\n",
    "            ϵ *= 0.99       \n",
    "        \n",
    "    print(f'Training ended. Number of wins: {wins}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you put everything together. It is almost completly finished for you. What values for the hyperparameters do you choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [-5.5931991e-01  2.6940505e-04]\n",
      "pos [-0.5228946  0.       ] posmin -1.2 bin 0.18 \n",
      "\n",
      "posindex, [3.7616966 6.6666665]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m num_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# number of training games\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# train the agent and store results\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mqLearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mα\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mγ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mϵ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mϵ_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrun1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, Q)\n",
      "Cell \u001b[1;32mIn[38], line 23\u001b[0m, in \u001b[0;36mqLearn\u001b[1;34m(Q, α, γ, ε, ε_min, num_games)\u001b[0m\n\u001b[0;32m     20\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m#take a random action\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# else choose the best know from the Q table\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     s_index \u001b[38;5;241m=\u001b[39m \u001b[43ms2q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[s_index])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mstep(action))    \n",
      "Cell \u001b[1;32mIn[37], line 25\u001b[0m, in \u001b[0;36ms2q\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     23\u001b[0m pos_index \u001b[38;5;241m=\u001b[39m (pos\u001b[38;5;241m-\u001b[39mpos_min) \u001b[38;5;241m/\u001b[39m bin_width_pos\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposindex,\u001b[39m\u001b[38;5;124m\"\u001b[39m,pos_index)\n\u001b[1;32m---> 25\u001b[0m pos_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpos_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m vel_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((vel\u001b[38;5;241m-\u001b[39mvel_min) \u001b[38;5;241m/\u001b[39m bin_width_vel )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#clipping\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# initialize the Q matrix as a numpy array with zeros\n",
    "Qdim = (10,10,env.action_space.n)\n",
    "Q = np.zeros(shape=Qdim)\n",
    "\n",
    "# set the hyperparameters\n",
    "α = 0.1  # learning rate [0,1]\n",
    "γ = 0.90  # discount rate [0,1]\n",
    "ϵ =  0.9 # epsilon greedy strategy [0,1]\n",
    "ϵ_min = 0.1  # minimu value of epsilon [0,1]\n",
    "num_games = 10 # number of training games\n",
    "\n",
    "# train the agent and store results\n",
    "qLearn(Q, α, γ, ϵ, ϵ_min, num_games)\n",
    "np.save('qrun1.npy', Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a Q-matrix has been trained we can use it as a policy and play a game. Write code that performs actions according to the input Q-matrix to play a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay the game using the trained Q matrix\n",
    "Q = np.load('qrun1.npy')\n",
    "\n",
    "# create and reset the environment with render mode on\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state = env.reset()  \n",
    "    \n",
    "# play a single episode with max. 1000 actions\n",
    "for _ in range(1000):           \n",
    "    env.render()                \n",
    "    loc = s2q(state)\n",
    "    # a = np.argmax(Q[#----------ADD CODE HERE---------#]) \n",
    "    # state, reward, terminated, truncated, info = env.step(a) \n",
    "        \n",
    "    if terminated: \n",
    "        print('Qplay Output:', reward, terminated, truncated, info)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Set up a couple fo experiments to figure oyt the following things:\n",
    "* How do $\\alpha$ and $\\gamma$ effect your learning perfomance?\n",
    "* Are both elements of the state vector equally important and can we reduce the dimensions of the Q-matrix of one (or both) of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
