{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Logistic Regression 2024  --- Part I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "%pip install numpy matplotlib seaborn pandas scikit-learn #installs them if they are not yet there\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "# to get matplot figures render correctly in the notebook use:\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Let's start by thanking the creators of the dataset that we will be using:\n",
    "\n",
    "Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009). Wine Quality. UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Loading and Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####INSTRUCTOR PART#####\n",
    "rwine = pd.read_csv('winequality-red.csv', sep=';')  # Makes Pandas objects\n",
    "wwine = pd.read_csv('winequality-white.csv', sep=';')  \n",
    "fig,ax= plt.subplots(ncols=6, nrows=2,figsize=(20,10))\n",
    "index=0\n",
    "ax=ax.flatten()\n",
    "for col, value in wwine.items():\n",
    "    sns.histplot(value, bins=20,ax=ax[index])\n",
    "    index+=1\n",
    "index=0\n",
    "for col, value in rwine.items():\n",
    "    sns.histplot(value, bins=20,ax=ax[index])\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The first step in all machine learning problems is to load and inspect the data. First, let's load the data.   \n",
    "The datafile `winequality-red.csv` is a ;-separated file that contains data of 1599 scientific meaurments of red wine. The file contains 11 columns which we will use as the features to describe the quality of the wine (last column).\n",
    "The datafile `winequality-white.csv` is a similar file but contains data of 4898 scientific meaurments of ___white___ wine. \n",
    "\n",
    "Use the code snippet below to load the data into a Pandas object `rwine` and `wwine`. ___We also add an additional column stating the color of the wine red(1)/white(0).___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PATH to the training data\n",
    "rwine = pd.read_csv('winequality-red.csv', sep=';')  # Makes Pandas objects\n",
    "wwine = pd.read_csv('winequality-white.csv', sep=';')  \n",
    "rwine.insert(12, \"color\", np.ones(len(rwine)), True)\n",
    "#rwine.head()\n",
    "wwine.insert(12,\"color\", np.zeros(len(wwine)), True)\n",
    "rwine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "\n",
    "Inspect the data by looking at the types of data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This prints how many measurements of wines are in our database:\n",
    "shape = wwine.shape\n",
    "print(f'shape = {shape}')\n",
    "#wwine.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "What we see is that that the 11 features are real numerical values (floats of 64 bits) and the quality of the wine has been rated with an integer mark.\n",
    "\n",
    "Now, inspect the numbers by e.g. printing the shape of `data` and the first couple of rows (this is called the header)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Research question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In this notebook we will several features available to us. Our research question is:\n",
    "\n",
    "__Based on the scientific measurments and the quality can we classify a wine to be red or white?__\n",
    "\n",
    "The first question that we have to solve is which ___two___ features (x1,x2) say the about the wine being red or white? Use the descibe function of pandas to ask for statistics and motivate your choice on the basis of those numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some statistics about our data:\n",
    "print(rwine.describe())\n",
    "print(wwine.describe())\n",
    "# Which two features will you pick to predict red/white wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a plot of the data belonging to those two features.\n",
    "# make a scatter plot the values of feature 1 on the x-axis and that of feature 2 on the y-axis\n",
    "# plot a blue marker for white and a red marker for red wines\n",
    "\n",
    "f1='...'\n",
    "f2='...'\n",
    "\n",
    "plt.scatter(wwine[f1],wwine[f2], c='b', marker='o', label='Red wine')\n",
    "plt.scatter(rwine[f1],rwine[f2], c='r', marker='o', label='White wine')\n",
    "plt.legend()\n",
    "plt.xlabel('fearure 1')\n",
    "plt.ylabel('feature 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Create combined red/white wine dataset and split in training and test set\n",
    "\n",
    "Combine both red and white wine datasets in a new set and **shuffle** the samples using pandas functions. Take the first 70\\% of your suffled set and assigne it the training set and the the remaining 30\\% to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine red and white wine pandas object into one new shuffled dataset:\n",
    "wines=pd.concat([wwine,rwine])\n",
    "wines = wines.sample(frac = 1)\n",
    "\n",
    "# the number of samples (wines) M in the total combined set\n",
    "M = len(wines)\n",
    "# the number of features N (excluding the bias)\n",
    "N = 2\n",
    "\n",
    "# Split data in 70% train &  30% test sets:\n",
    "\n",
    "# the number of samples M in the training and test set\n",
    "Mtrain = int(0.7*M)\n",
    "Mtest = M-Mtrain\n",
    "print(M,Mtrain,Mtest)\n",
    "\n",
    "#Split the shuffled \n",
    "x1data_train=wines[f1][0:Mtrain]       # data of your selected feature 1\n",
    "x2data_train=wines[f2][0:Mtrain]       # data of your selected feature 2\n",
    "label_train= wines['color'][0:Mtrain]  # label red(1)/white(0) wine\n",
    "x1data_test= ...\n",
    "x2data_test= ...\n",
    "label_test= ...:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Feature scaling (important!)\n",
    "\n",
    "Before we can setup the logistic regression model it is important to apply feature scaling to each of our selected feature vectors. For this you need to compute the _mean_ ($\\mu$) and the _standard deviation_ $(\\sigma_{\\rm std})$ of each feature vector and apply the following scaling (leture notes Eq. (3.15)):\n",
    "\n",
    "$$  \\mathbf{x}=\\frac{ \\mathbf{x} -\\mu}{\\sigma_{\\rm std}}$$ \n",
    "\n",
    "Write below one function based on input $(\\mu,\\sigma_{\\rm std})$ to feature scale a vector and one to undo (inverse) the feacture scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#write a forward and a backwards function that does your feature scaling\n",
    "def fscale(x,mean,std):\n",
    "    \"\"\"\n",
    "    returns the feature scaled vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray of shape (M, 1)\n",
    "        feature matrix.\n",
    "    mean : float\n",
    "    std : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scaled x : same type and shape as x\n",
    "\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "def fscale_inv(x,mean,std):\n",
    "    \"\"\"\n",
    "    returns the vector x\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scaled x : ndarray of shape (M, 1)\n",
    "        feature matrix.\n",
    "    mean : float\n",
    "    std : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : same type and shape as scaled x\n",
    "\n",
    "    \"\"\"\n",
    "    return ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Compute the _mean_ and _std_ of the features in your training data and use the to scale both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the mean and the std of features x1 and x2 of your training and test sets:\n",
    "mean1=np.mean(x1data_train)\n",
    "std1= ...\n",
    "mean2= ...\n",
    "std2= ...\n",
    "mean1test= ...\n",
    "std1test= ...\n",
    "mean2test= ...\n",
    "std2test= ...\n",
    "\n",
    "# Verify that the means and std of the training and test sets are comparable:\n",
    "print('means_train:',mean1,mean2)\n",
    "print('means_test:',mean1test,mean2test)\n",
    "print('stds_train:',std1,std2)\n",
    "print('stds_test:',std1test,std2test)\n",
    "\n",
    "#Feature scale training and test set using the means and stds obtained on the training set.\n",
    "x1data_train= ...\n",
    "x2data_train= ...\n",
    "x1data_test= ...\n",
    "x2data_test= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We will build a logistic model to define a boarder line between the red and white wines. The classification is then based on wheter the model predicts a certain set of f1/f2 numbers to be either on the left or the right side of this line.\n",
    "\n",
    "The logistic regression model is given by (lecture notes eq. 3.4):\n",
    "$$  \\mathbf{h}_{\\theta} (\\mathbf{X})  = \\sigma (\\mathbf{X} \\boldsymbol{\\theta})$$ \n",
    "with $\\mathbf{X}$ the feature matrix , $\\mathbf{h}$ the prediction (as a value between 0 and 1), $\\boldsymbol{\\theta}$ the weight vector and the function $\\sigma()$ the logistic function given by:\n",
    "$$ \\sigma (z) = \\frac{1}{1 + e^{-z}}   $$\n",
    "\n",
    "The weight vector is found by minimizing the *loss* $L$. The loss in this case is given by (lecture notes eq. 3.9):\n",
    "$$  L = - \\frac{1}{M} \\left ( \\mathbf{y}^T \\ln \\left [ \\sigma \\left ( \\mathbf{X}\\boldsymbol{\\theta} \\right ) \\right ] + \\left ( \\mathbf{1} - \\mathbf{y} \\right )^T  \\ln \\left [\\mathbf{1} - \\sigma \\left ( \\mathbf{X}\\boldsymbol{\\theta} \\right ) \\right ]  \\right ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Create the feature matrix and label vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The feature matrix is represented by a 2D numpy array `X` composed of a column filled with ones (the bias) and column(s) with the feature(s). Also we need to define the label vector y as a 2D numpy array `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the feature matrix as a ( Mtrain x N+1 ) ndarray\n",
    "X = \n",
    "# define the label vector as a (Mtrain x 1) ndarray\n",
    "y=\n",
    "# define some intital guess for the theta vector as a (N+1 x 1) ndarray\n",
    "theta =\n",
    "print(y.shape,X.shape, theta.shape) # Should result (4547, 1) (4547, 3) (3, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Since we will need the logistic function let's implement a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    returns the sigmoid function of z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : float or ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    1 / (1 + exp(-z)) : same type and shape as z\n",
    "\n",
    "    \"\"\"\n",
    "    z = np.float128(z) # to increase precision of the variable z\n",
    "    \n",
    "    return ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Implement the loss function\n",
    "Next, define a function that computes the loss given the feature matrix, the label vector and the weight vector.\n",
    "\n",
    "function | description\n",
    "----|----\n",
    "`compute_loss(X, y, theta)` | This function takes the feature matrix `X`, label vector `y` and the weight vector `theta` and returns the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the loss (or cost)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (M, N+1)\n",
    "        feature matrix.\n",
    "    y : ndarray of shape (M, 1)\n",
    "        label vector.\n",
    "    theta : ndarray of shape (M, 1)\n",
    "        weights vector.        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Loss : float\n",
    "\n",
    "    \"\"\"\n",
    "    M = len(y)\n",
    "    \n",
    "    loss = ...\n",
    "    \n",
    "    # check if loss has a valid value, if not, let loss be infinite\n",
    "    if np.isnan(loss):\n",
    "        loss = np.inf\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the function with some value for theta\n",
    "theta = ...\n",
    "loss = ...\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The next step is to train the model. We use the gradient descent algortihm for this. The weight vector is updated according to:\n",
    "$$  \\boldsymbol{\\theta} \\rightarrow \\boldsymbol{\\theta} - \\eta \\frac{\\mathrm{d} L}{\\mathrm{d} \\boldsymbol{\\theta}} $$\n",
    "where $\\eta$ is a *hyperparameter* that allows to tune the rate of descent, and the gradient defined as (lecture notes eq. 3.10) ___note the small difference wrt. linear regression of last week___:\n",
    "\n",
    "$$  \\frac{\\mathrm{d} L}{\\mathrm{d} \\boldsymbol{\\theta}}  = \\frac{1}{M} \\mathbf{X}^T  \\left ( \\sigma \\left ( \\mathbf{X}  \\boldsymbol{\\theta} \\right ) - \\textbf y \\right ) $$\n",
    "\n",
    "Therefore we need to implement the following functions:\n",
    "\n",
    "function | description\n",
    "--------|----\n",
    "`gradient(X, y, theta)` | This function takes the feature matrix `X`, label vector `y` and the weight vector `theta` and returns the gradient of the loss w.r.t. the weight(s)\n",
    "`train(X, y, theta, eta, num_iters)` | This function takes the feature matrix `X`, label vector `y`, the weight vector `theta`, hyperparameter `eta` and the number of iteration steps `num_iters`, and returns the weight vector the minimises the loss.\n",
    "\n",
    "__Minimizing the logistic loss by direct computation?__\n",
    "Note that in logistic regression we cannot minimize the loss by a direct computation. We therefore have to always have to optimize theta via gradual optimisation methods, such as gradient desent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (M, N+1)\n",
    "        feature matrix.\n",
    "    y : ndarray of shape (M, 1)\n",
    "        label vector.\n",
    "    theta : ndarray of shape (M, 1)\n",
    "        weights vector.        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : ndarray of shape (N+1, 1)\n",
    "        the gradient\n",
    "\n",
    "    \"\"\"\n",
    "    M = len(y)\n",
    "    grad = ...\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the function for some value of theta\n",
    "theta = ...\n",
    "grad = ...\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, define the function `train()` that implements the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X, y, theta, eta = 0.01, num_iters = 7500):\n",
    "    \"\"\"\n",
    "    Implements the gradient descent algorithm for a linear regression model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (M x N+1) ndarray (with M the number of samples and N the number of features excluding the bias)\n",
    "        The feature matrix including the bias in the first column\n",
    "    y : (M x 1) ndarray\n",
    "        label vector\n",
    "    theta : (N+1 x 1) ndarray\n",
    "        The weight vector inlcuding the bias\n",
    "    eta : float, optional\n",
    "        The rate. The default is 0.01.\n",
    "    num_iters : int, optional\n",
    "        Number of iterations that is performed. The default is 7500.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : (N+1 x 1) ndarray\n",
    "        The optimised weight vector    \n",
    "    loss : (num_iters,) ndarray\n",
    "        The loss at each iteration\n",
    "    \"\"\"\n",
    "    theta = ...\n",
    "    loss = ...\n",
    "\n",
    "    return (theta, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intialize theta to some value\n",
    "theta = ...\n",
    "# train the model\n",
    "theta, loss = ....\n",
    "# print the computed weight vector\n",
    "print(f'weigths: {theta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "First have a (critical) look at how the loss changed during the training by plotting the loss versus the iteration number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a plot of the loss versus the interation number\n",
    "plt.plot(loss)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "To evaluate the model, we need a function `predict(X, theta, threshold=0.5)` that predicts the label `y` given some new values of the features `X` and a `threshold` value that determines the decision boundary (defaults to 0.5). Implement this function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, theta, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Predicts the label y given features X for a logistic regression model with weights theta\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (M x N+1) ndarray (with M the number of samples and N the number of features excluding the bias)\n",
    "        The feature matrix including the bias in the first column \n",
    "    theta : (N+1 x 1) ndarray\n",
    "        The weight vector inlcuding the bias\n",
    "    threshold : float (default=0.5\n",
    "        the threshold value of the decision boundary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : (M x 1) ndarray\n",
    "        The predicted labels (either 0 or 1)    \n",
    "    \"\"\"  \n",
    "    prediction = ...\n",
    "    y_pred = prediction.astype('int')  # convert to integer (0 or 1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Benchmark the validity of your model with respect to the test set\n",
    "\n",
    "First, to see how well the model predicts the correct label compute the percentage of correct predictions for the training set.\n",
    "Compute the prediction error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute how many % of the predictions are correct with your current model\n",
    "ypred = ...\n",
    "print(f'Training accuracy : {100 * np.mean(y==ypred)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Scond, Compute the prediction error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the feature matrix as a ( Mtest x N+1 ) ndarray\n",
    "Xtest = ...   \n",
    "# define the label vector as a (Mtest x 1) ndarray\n",
    "ytest = ...\n",
    "# compute how many % of the test set predictions are correct with your current model\n",
    "p = ...\n",
    "print(f'Test accuracy : {100 * np.mean(ytest==p)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "\n",
    "At this point it is possible that your model has a higher accuracy on the training set than on the test set. This means that your model is over-fitted (over specialised) on that dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Visualise the decision boundary\n",
    "You have to plot the training and test data together with the decision boundary (similar to figure 3.3. of the reader).  Below you will create a rectangular grid with the width based on the min and max values in you features 1 and 2. Apply your model to predit the label on this grid. The decision boundary lies at 0.5, ___why?__, _if you were to plot two lines at 0.3 and 0.7 what would your model say about the samples that lie inbetween?_\n",
    "\n",
    "**Do not forget to invert the effects of the feature scaling before plotting your results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a scatter plot the values of feature 1 on the x-axis and that of feature 2 on the y-axis\n",
    "\n",
    "# Create a mask from you labels where color=1 and color=0:\n",
    "mask1 = label_train[:] == 1\n",
    "mask2 = label_train[:] == 0\n",
    "# Inverse the feature scaling on x1data_train and x2data_train and call the results d1 and d2:\n",
    "d1= ...\n",
    "d2= ...\n",
    "# plot a blue marker for white and a red marker for red wines in the training set\n",
    "plt.scatter(d1[mask1], d2[mask1], c='b', marker='o', label='Red wine (train)')\n",
    "plt.scatter(d1[mask2], d2[mask2], c='r', marker='o', label='White wine (train)')\n",
    "\n",
    "# The same as above but now for the test set:\n",
    "mask1 = ...\n",
    "mask2 = ...\n",
    "d1= ...\n",
    "d2= ...\n",
    "plt.scatter(d1[mask1], d2[mask1], c='k', s=10, marker='o', label='Red wine (test)')\n",
    "plt.scatter(d1[mask2], d2[mask2], c='y', s=10, marker='o', label='White wine (test)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "\n",
    "# the code below plots the decision boundary in the plot. This is a bit of an overkill for our current simple (straight line) model,\n",
    "# but can also be used in the same wat when you add higher order polynimal features in the Design matrix.\n",
    "\n",
    "# build mesh of gridpoints\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max()\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max()\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# compute hypothesis on the meshgrid \n",
    "# first create a Mx2 ndarray with the values of the scores on the mesh \n",
    "data_mesh = np.column_stack((xx1.ravel(), xx2.ravel()))\n",
    "# compute the full feature matrix\n",
    "bias = np.ones(len(data_mesh)).reshape(-1,1)\n",
    "X_mesh = np.column_stack((bias, data_mesh))\n",
    "# compute the predicted labels (h) with you model on this mesh (do not force integers as the predict function does):\n",
    "h = sigmoid(X_mesh @ theta)\n",
    "h = h.reshape(xx1.shape)\n",
    "\n",
    "# Before plotting the contour levels you should undo the feature scaling on the grids xx1,xx2:\n",
    "xx1= ...\n",
    "xx2= ...\n",
    "\n",
    "# plot the contour level(s)\n",
    "plt.contour(xx1, xx2, h, [0.5], linewidths=5, colors='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Logistic Regression 2024  --- Part II --- The full monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Lets see how far we can go in our attempt to make 'a near 100\\%' ( a sales person would say: fool proof) classification model of white/red wines. We will use all avalaible data, including wine quality, to predict the color of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have our combined and shuffled dataset wines:\n",
    "wines = wines.sample(frac = 1)\n",
    "# We just drop the 'color' from the dataset:\n",
    "wines_nocolor=wines.drop('color', axis=1)\n",
    "\n",
    "# the number of samples M in the total combined set is still the same\n",
    "M = len(wines_nocolor)\n",
    "# the number of features N (excluding the bias) are now all (including f='quality'):\n",
    "N = ...\n",
    "wines_nocolor.head() # We will use everything you see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in 70% train &  30% test sets:\n",
    "\n",
    "# the number of samples M in the training and test set\n",
    "Mtrain = int(0.7*M)\n",
    "Mtest = M-Mtrain\n",
    "print(M,Mtrain,Mtest)\n",
    "\n",
    "#Split the shuffled dataset wines_nocolor. We will immediately define the 2 design matrices and the label vectors\n",
    "#for the train and test set.\n",
    "\n",
    "#First define numpy ones matrices with the appropriate dimensions\n",
    "X_train=np.ones((Mtrain,N+1))\n",
    "y_train=np.ones((Mtrain,1))\n",
    "X_test= ...\n",
    "y_test= ...\n",
    "#Second fill those matrices with the feature values\n",
    "X_train[:,1:N+1]=wines_nocolor[:][0:Mtrain]     # data of all features in training set\n",
    "y_train[:,0]= wines['color'][0:Mtrain]          # label red(1)/white(0) wine\n",
    "X_test[:,1:N+1]= ...                            # data of all features in training set\n",
    "y_test[:,0]= ...                                # label red(1)/white(0) wine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "We have to feature scale the design matrix $\\mathbf{X_{\\rm train}}$ and $\\mathbf{X_{\\rm test}}$. You can use the function below to do that or write something yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function feature scales the entire design matrix X (except for the first columns with ones)\n",
    "def fscale_X(X):\n",
    "    N=X.shape[1]-1\n",
    "    means=np.zeros(N)\n",
    "    stds=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        means[i]=np.mean(X[:,i+1])\n",
    "        stds[i]=np.std(X[:,i+1])\n",
    "        X[:,i+1]=fscale(X[:,i+1],means[i],stds[i])\n",
    "    return (X,means,stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,means_train,stds_train=fscale_X(X_train)\n",
    "X_test,means_test,stds_test=fscale_X(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Build a linear and a logistic Regression model\n",
    "$$\\mathbf{y}_{\\rm pred\\_lin}=\\mathbf{X_{\\rm train}}\\boldsymbol{\\theta_{\\rm lin}}$$\n",
    "$$\\mathbf{y}_{\\rm pred\\_log}= \\sigma \\left(\\mathbf{X_{\\rm train}} \\boldsymbol{\\theta_{\\rm log}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use eq. (2.14) from the lecture notes to compute the theta (weights) of the linear model\n",
    "theta_lin=theta = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your gradient descent approach as coded above to compute the theta (weights) logistic model\n",
    "theta_log, loss = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute how many % of the test set predictions are correct with your linear regression model\n",
    "p = ...\n",
    "y_pred_lin = p.astype('int')  # convert to integer (0 or 1)\n",
    "print(f'Test accuracy linear regression : {100 * np.mean(y_test==y_pred_lin)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute how many % of the test set predictions are correct with your logistic regression model\n",
    "y_pred_log = ...\n",
    "print(f'Test accuracy logistic regression: {100 * np.mean(y_test==y_pred_log)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity check:\n",
    "print(y_test.astype('int')[0:20].T)\n",
    "print(y_pred_lin[0:20].T)\n",
    "print(y_pred_log[0:20].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### Analyse and interpret\n",
    "Before stopping, take a moment to think about the meaning of the values in the theta (weights) vector. As we have performed feature scaling we can interpret the absolute values of the theta vetor as a weight. The larger the weight the more important that feature is in your classification model. Did your linear model and logistic model agree on what the important features are? **The two features that you choose in the start of this notebook, are they the same as your ML models determined to be important?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wines_nocolor.dtypes)\n",
    "print(theta_lin.T)\n",
    "print(theta_log.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "\n",
    "## The end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10",
   "language": "python",
   "name": "py3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
